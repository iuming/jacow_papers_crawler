#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŒ‰ä¼šè®®ä¸‹è½½æ‰€æœ‰å•ç¯‡è®ºæ–‡çš„ä¸“ç”¨å·¥å…·
"""

import asyncio
import aiohttp
import re
from pathlib import Path
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import sys


async def download_conference_papers(conference_name, year, max_size_mb=50):
    """
    ä¸‹è½½æŒ‡å®šä¼šè®®çš„æ‰€æœ‰å•ç¯‡è®ºæ–‡

    Args:
        conference_name: ä¼šè®®åç§° (å¦‚ 'IPAC', 'LINAC')
        year: å¹´ä»½ (å¦‚ 2023)
        max_size_mb: æœ€å¤§æ–‡ä»¶å¤§å°é™åˆ¶(MB)
    """
    print(f"ğŸ¯ å‡†å¤‡ä¸‹è½½ {conference_name} {year} ä¼šè®®çš„æ‰€æœ‰å•ç¯‡è®ºæ–‡")
    print(f"ğŸ“ æ–‡ä»¶å¤§å°é™åˆ¶: {max_size_mb}MB")
    print("=" * 60)

    # æ„å»ºä¼šè®®URL
    conference_url = f"https://proceedings.jacow.org/{conference_name.lower()}{year}/"
    session_index_url = urljoin(conference_url, "session/index.html")

    async with aiohttp.ClientSession() as session:
        print(f"ğŸŒ è®¿é—®ä¼šè®®ä¸»é¡µ: {conference_url}")

        # è·å–sessionåˆ—è¡¨
        try:
            async with session.get(session_index_url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, "html.parser")

                    # æŸ¥æ‰¾æ‰€æœ‰sessioné“¾æ¥
                    session_links = []
                    for link in soup.find_all("a", href=True):
                        href = link["href"]
                        if (
                            "session" in href
                            and "index.html" in href
                            and link.get_text(strip=True)
                        ):
                            full_url = urljoin(session_index_url, href)
                            session_links.append(
                                {"name": link.get_text(strip=True), "url": full_url}
                            )

                    print(f"âœ… æ‰¾åˆ° {len(session_links)} ä¸ªsession")

                    # éå†æ¯ä¸ªsessionè·å–è®ºæ–‡
                    all_papers = []
                    for i, session_info in enumerate(session_links, 1):
                        print(
                            f"\nğŸ“‘ å¤„ç†Session {i}/{len(session_links)}: {session_info['name']}"
                        )

                        try:
                            async with session.get(
                                session_info["url"]
                            ) as session_response:
                                if session_response.status == 200:
                                    session_html = await session_response.text()
                                    papers = extract_papers_from_session(
                                        session_html, session_info["url"]
                                    )

                                    if papers:
                                        print(f"   ğŸ“„ æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
                                        all_papers.extend(papers)
                                    else:
                                        print(f"   âš ï¸ æœªæ‰¾åˆ°è®ºæ–‡")
                                else:
                                    print(
                                        f"   âŒ æ— æ³•è®¿é—® (çŠ¶æ€ç : {session_response.status})"
                                    )
                        except Exception as e:
                            print(f"   âŒ å¤„ç†å‡ºé”™: {e}")

                        # é¿å…è¯·æ±‚è¿‡å¿«
                        await asyncio.sleep(1)

                    print(f"\nğŸ‰ æ€»å…±æ‰¾åˆ° {len(all_papers)} ç¯‡å•ç‹¬è®ºæ–‡")

                    # æ˜¾ç¤ºå‰10ç¯‡è®ºæ–‡ä½œä¸ºç¤ºä¾‹
                    print(f"\nğŸ“‹ è®ºæ–‡åˆ—è¡¨é¢„è§ˆï¼ˆå‰10ç¯‡ï¼‰:")
                    for i, paper in enumerate(all_papers[:10], 1):
                        print(f"{i:2d}. {paper['code']} - {paper['title'][:60]}...")
                        print(f"    URL: {paper['url']}")

                    if len(all_papers) > 10:
                        print(f"    ... è¿˜æœ‰ {len(all_papers) - 10} ç¯‡è®ºæ–‡")

                    return all_papers
                else:
                    print(f"âŒ æ— æ³•è®¿é—®ä¼šè®®é¡µé¢ (çŠ¶æ€ç : {response.status})")
                    return []
        except Exception as e:
            print(f"âŒ è·å–ä¼šè®®ä¿¡æ¯å¤±è´¥: {e}")
            return []


def extract_papers_from_session(html, base_url):
    """ä»sessioné¡µé¢æå–è®ºæ–‡ä¿¡æ¯"""
    soup = BeautifulSoup(html, "html.parser")
    papers = []

    # æŸ¥æ‰¾æ‰€æœ‰PDFé“¾æ¥
    pdf_links = soup.find_all("a", href=lambda href: href and href.endswith(".pdf"))

    for link in pdf_links:
        href = link.get("href")
        if href and is_individual_paper(href):
            # æ„å»ºå®Œæ•´URL
            if href.startswith("http"):
                paper_url = href
            else:
                paper_url = urljoin(base_url, href)

            # æå–è®ºæ–‡ä»£ç 
            paper_code = href.split("/")[-1].replace(".pdf", "")

            # æå–æ ‡é¢˜ï¼ˆç®€åŒ–ç‰ˆï¼‰
            title = extract_title_from_context(link, paper_code)

            papers.append(
                {
                    "code": paper_code,
                    "title": title,
                    "url": paper_url,
                    "session": extract_session_from_url(base_url),
                }
            )

    return papers


def is_individual_paper(url):
    """åˆ¤æ–­æ˜¯å¦æ˜¯å•ç¯‡è®ºæ–‡"""
    filename = url.split("/")[-1].lower()

    # æ’é™¤è®ºæ–‡é›†
    exclude_patterns = ["proceedings", "complete", "full", "volume", "brief"]
    if any(pattern in filename for pattern in exclude_patterns):
        return False

    # å•ç¯‡è®ºæ–‡æ¨¡å¼
    import re

    individual_patterns = [
        r"^[A-Z]{2,4}[A-Z0-9]{2,6}\.pdf$",
        r"^[A-Z]{2,6}\d{2,4}\.pdf$",
    ]

    for pattern in individual_patterns:
        if re.match(pattern, filename, re.IGNORECASE):
            return True

    return len(filename) < 20 and bool(re.search(r"\d", filename))


def extract_title_from_context(link, paper_code):
    """ä»ä¸Šä¸‹æ–‡æå–è®ºæ–‡æ ‡é¢˜"""
    # ç®€åŒ–çš„æ ‡é¢˜æå–
    parent = link.parent
    if parent:
        parent_text = parent.get_text(strip=True)
        # æŸ¥æ‰¾è®ºæ–‡ä»£ç åçš„æ–‡æœ¬ä½œä¸ºæ ‡é¢˜
        code_index = parent_text.find(paper_code)
        if code_index >= 0:
            title_start = code_index + len(paper_code)
            title = parent_text[title_start:].strip()
            # æ¸…ç†æ ‡é¢˜
            title = re.sub(r"^[^\w]*", "", title)  # å»æ‰å¼€å¤´çš„æ ‡ç‚¹
            title = title.split("â– ")[0]  # å»æ‰ä½œè€…ä¿¡æ¯
            title = title.split("DOI:")[0]  # å»æ‰DOIä¿¡æ¯
            if len(title) > 10:
                return title[:100]  # é™åˆ¶é•¿åº¦

    return paper_code


def extract_session_from_url(url):
    """ä»URLæå–sessionä¿¡æ¯"""
    if "/session/" in url:
        session_part = url.split("/session/")[-1]
        session_code = session_part.split("/")[0]
        session_match = re.search(r"(\d+-)?(.+)", session_code)
        if session_match:
            return session_match.group(2).upper()
    return "unknown"


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ JACoWä¼šè®®å•ç¯‡è®ºæ–‡æ‰¹é‡ä¸‹è½½å·¥å…·")
    print("=" * 60)

    if len(sys.argv) < 3:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("python conference_downloader.py <ä¼šè®®å> <å¹´ä»½> [æœ€å¤§æ–‡ä»¶å¤§å°MB]")
        print()
        print("ç¤ºä¾‹:")
        print("python conference_downloader.py IPAC 2023")
        print("python conference_downloader.py IPAC 2023 30")
        print("python conference_downloader.py LINAC 2022 50")
        return

    conference = sys.argv[1].upper()
    year = int(sys.argv[2])
    max_size = int(sys.argv[3]) if len(sys.argv) > 3 else 50

    papers = await download_conference_papers(conference, year, max_size)

    if papers:
        print(f"\nâœ… æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡ä¿¡æ¯")
        print(f"\nğŸ’¡ ä½ ç°åœ¨å¯ä»¥ç”¨è¿™äº›URLæ¥ä¸‹è½½è®ºæ–‡ï¼š")
        print(f"   - ä½¿ç”¨ç°æœ‰çš„ä¸‹è½½å™¨")
        print(f"   - æˆ–è€…æ·»åŠ å®é™…ä¸‹è½½åŠŸèƒ½")
        print(f"\nğŸŠ {conference} {year} ä¼šè®®å•ç¯‡è®ºæ–‡è·å–å®Œæˆï¼")
    else:
        print(f"\nâŒ æœªèƒ½è·å–åˆ°è®ºæ–‡ä¿¡æ¯")


if __name__ == "__main__":
    asyncio.run(main())
