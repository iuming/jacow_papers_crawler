#!/usr/bin/env python3
"""
ä»ç½‘é¡µå†…å®¹ä¸­è§£æsessioné“¾æ¥
åŸºäºä¹‹å‰è·å–çš„å®é™…ç½‘é¡µå†…å®¹
"""

import re
import asyncio
import aiohttp
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import sys


async def extract_sessions_from_content():
    """ä»HTMLå†…å®¹ä¸­æå–sessionä¿¡æ¯"""
    base_url = "https://proceedings.jacow.org/ipac2023"
    session_url = f"{base_url}/session/index.html"

    async with aiohttp.ClientSession() as session:
        print(f"ğŸ” æ­£åœ¨åˆ†æ: {session_url}")

        try:
            async with session.get(session_url) as response:
                if response.status == 200:
                    content = await response.text()
                    print(f"âœ… é¡µé¢å¤§å°: {len(content)} å­—ç¬¦")

                    # æŸ¥æ‰¾sessionæ–‡æœ¬å†…å®¹ï¼Œå®ƒä»¬åŒ…å«åœ¨HTMLä¸­
                    # åŸºäºä¹‹å‰çœ‹åˆ°çš„æ¨¡å¼ï¼š[MOPA - Monday Poster Session: MOPA]
                    session_pattern = r"\[([A-Z]{2,6})\s*-\s*([^]]+)\]\(([^)]+)\)"
                    matches = re.findall(session_pattern, content)

                    print(f"\nğŸ¯ æ‰¾åˆ°sessionæ¨¡å¼åŒ¹é…: {len(matches)}ä¸ª")
                    for i, (code, description, url) in enumerate(matches[:10], 1):
                        print(f"  {i}. {code}: {description[:30]}... -> {url}")

                    # å¦ä¸€ç§æ–¹æ³•ï¼šæŸ¥æ‰¾ç‰¹å®šçš„hrefæ¨¡å¼
                    # åŸºäºçœ‹åˆ°çš„URLï¼šproceedings.jacow.org/ipac2023/session/238-mopa/index.html
                    href_pattern = r'href="([^"]*session/[^"]*index\.html)"'
                    href_matches = re.findall(href_pattern, content)

                    print(f"\nğŸ”— æ‰¾åˆ°hrefæ¨¡å¼åŒ¹é…: {len(href_matches)}ä¸ª")
                    for i, url in enumerate(href_matches[:10], 1):
                        print(f"  {i}. {url}")

                    # ç›´æ¥æœç´¢sessionæ•°å­—-å­—æ¯æ¨¡å¼
                    session_code_pattern = r"(\d+)-([a-z]+)/index\.html"
                    code_matches = re.findall(session_code_pattern, content)

                    print(f"\nğŸ“Š æ‰¾åˆ°sessionä»£ç : {len(code_matches)}ä¸ª")
                    sessions = []
                    for i, (number, code) in enumerate(code_matches[:15], 1):
                        session_path = f"{number}-{code}/index.html"
                        full_url = urljoin(f"{base_url}/session/", session_path)
                        sessions.append(full_url)
                        print(f"  {i}. {session_path} -> {full_url}")

                    return sessions

                else:
                    print(f"âŒ HTTP {response.status}")
                    return []

        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            return []


if __name__ == "__main__":
    if sys.platform.startswith("win"):
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

    sessions = asyncio.run(extract_sessions_from_content())
    print(f"\nğŸ‰ æ€»å…±æ‰¾åˆ° {len(sessions)} ä¸ªsessions")
