#!/usr/bin/env python3
"""
è°ƒè¯•JACoW sessioné“¾æ¥æå–
"""

import asyncio
import aiohttp
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import sys

async def debug_session_links():
    """è°ƒè¯•sessioné“¾æ¥æå–"""
    base_url = "https://proceedings.jacow.org/ipac2023"
    session_url = f"{base_url}/session/index.html"
    
    async with aiohttp.ClientSession() as session:
        print(f"ğŸ” æ­£åœ¨è°ƒè¯•: {session_url}")
        
        try:
            async with session.get(session_url) as response:
                if response.status == 200:
                    content = await response.text()
                    print(f"âœ… é¡µé¢å¤§å°: {len(content)} å­—ç¬¦")
                    
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                    all_links = []
                    for link in soup.find_all('a', href=True):
                        href = link.get('href', '')
                        text = link.get_text(strip=True)
                        all_links.append((href, text))
                    
                    print(f"\nğŸ“„ æ‰€æœ‰é“¾æ¥:")
                    for i, (href, text) in enumerate(all_links, 1):
                        print(f"  {i}. '{href}' -> '{text[:30]}...'")
                    
                    # åˆ†æå¯èƒ½çš„sessioné“¾æ¥
                    potential_sessions = []
                    for href, text in all_links:
                        # å¯»æ‰¾ç¬¦åˆsessionæ¨¡å¼çš„é“¾æ¥ - æ›´å®½æ¾çš„åŒ¹é…
                        if href.endswith('.html') and href != 'index.html':
                            potential_sessions.append((href, text))
                    
                    print(f"\nğŸ¯ å¯èƒ½çš„sessioné“¾æ¥ ({len(potential_sessions)}ä¸ª):")
                    for i, (href, text) in enumerate(potential_sessions, 1):
                        print(f"  {i}. '{href}' -> '{text[:50]}...'")
                    
                    # ç‰¹åˆ«æŸ¥æ‰¾æ•°å­—-å­—æ¯æ¨¡å¼çš„é“¾æ¥
                    numeric_sessions = []
                    for href, text in all_links:
                        # å¯»æ‰¾ç±»ä¼¼ "221-supm/index.html" çš„æ¨¡å¼
                        if '/' in href and href.endswith('/index.html'):
                            numeric_sessions.append((href, text))
                    
                    print(f"\nğŸ”¢ æ•°å­—æ¨¡å¼sessioné“¾æ¥ ({len(numeric_sessions)}ä¸ª):")
                    for i, (href, text) in enumerate(numeric_sessions, 1):
                        print(f"  {i}. '{href}' -> '{text[:50]}...'")
                        if i >= 10:
                            print(f"  ... è¿˜æœ‰ {len(numeric_sessions) - 10} ä¸ª")
                            break
                    
                    # ç”Ÿæˆå®Œæ•´URL
                    if potential_sessions:
                        print(f"\nğŸŒ ç”Ÿæˆçš„å®Œæ•´session URLs:")
                        for i, (href, text) in enumerate(potential_sessions[:5], 1):
                            full_url = urljoin(f"{base_url}/session/", href)
                            print(f"  {i}. {full_url}")
                    
                else:
                    print(f"âŒ HTTP {response.status}")
                    
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")

if __name__ == "__main__":
    if sys.platform.startswith('win'):
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    asyncio.run(debug_session_links())
