#!/usr/bin/env python3
"""
æ­£ç¡®è§£æJACoW sessionä¸­çš„PDFé“¾æ¥
ä½¿ç”¨data-hrefå±æ€§
"""

import asyncio
import aiohttp
import re
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import sys


async def extract_papers_from_mopa():
    """ä»MOPA sessionæ­£ç¡®æå–PDFé“¾æ¥"""
    base_url = "https://proceedings.jacow.org/ipac2023"
    session_url = f"{base_url}/session/238-mopa/index.html"

    async with aiohttp.ClientSession() as session:
        print(f"ğŸ” æ­£ç¡®è§£æMOPA session: {session_url}")

        try:
            async with session.get(session_url) as response:
                if response.status == 200:
                    content = await response.text()
                    print(f"âœ… é¡µé¢å¤§å°: {len(content)} å­—ç¬¦")

                    soup = BeautifulSoup(content, "html.parser")

                    # æŸ¥æ‰¾å…·æœ‰data-hrefå±æ€§çš„é“¾æ¥
                    data_href_links = soup.find_all("a", attrs={"data-href": True})
                    print(f"\nğŸ”— æ‰¾åˆ° {len(data_href_links)} ä¸ªdata-hrefé“¾æ¥")

                    individual_papers = []
                    for link in data_href_links:
                        data_href = link.get("data-href", "")

                        # æ£€æŸ¥æ˜¯å¦ä¸ºå•ç¯‡PDFè®ºæ–‡
                        if data_href.endswith(".pdf") and "MOPA" in data_href:
                            # æ„å»ºå®Œæ•´URL
                            full_url = urljoin(session_url, data_href)

                            # æå–è®ºæ–‡ID
                            paper_id = data_href.split("/")[-1].replace(".pdf", "")

                            # è·å–è®ºæ–‡æ ‡é¢˜
                            title = link.get_text(strip=True) or paper_id

                            individual_papers.append(
                                {
                                    "id": paper_id,
                                    "url": full_url,
                                    "title": title,
                                    "relative_path": data_href,
                                }
                            )

                    print(f"\nğŸ“„ æ‰¾åˆ° {len(individual_papers)} ç¯‡å•ç‹¬è®ºæ–‡:")
                    for i, paper in enumerate(individual_papers[:10], 1):
                        print(f"  {i}. {paper['id']}: {paper['title'][:40]}...")
                        print(f"      URL: {paper['url']}")

                    if len(individual_papers) > 10:
                        print(f"  ... è¿˜æœ‰ {len(individual_papers) - 10} ç¯‡è®ºæ–‡")

                    return individual_papers

                else:
                    print(f"âŒ HTTP {response.status}")
                    return []

        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            return []


if __name__ == "__main__":
    if sys.platform.startswith("win"):
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

    papers = asyncio.run(extract_papers_from_mopa())
    print(f"\nğŸ‰ æˆåŠŸæå– {len(papers)} ç¯‡è®ºæ–‡")
