#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è°ƒè¯•å•ç¯‡è®ºæ–‡æå–åŠŸèƒ½
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

async def debug_paper_extraction():
    """è°ƒè¯•è®ºæ–‡æå–è¿‡ç¨‹"""
    print("ğŸ” è°ƒè¯•å•ç¯‡è®ºæ–‡æå–è¿‡ç¨‹")
    
    try:
        from crawler.individual_spider import JACoWIndividualPaperSpider
        from utils.logger import setup_logger
        from bs4 import BeautifulSoup
        
        # è®¾ç½®æ—¥å¿—
        logger = setup_logger("debug", level="DEBUG")
        
        # åˆå§‹åŒ–çˆ¬è™«
        spider = JACoWIndividualPaperSpider(delay=1.0, logger=logger)
        
        # æµ‹è¯•é¡µé¢
        session_url = "https://proceedings.jacow.org/ipac2023/session/238-mopa/index.html"
        print(f"\nğŸ“„ è·å–é¡µé¢å†…å®¹: {session_url}")
        
        html = await spider._fetch_page(session_url)
        if html:
            print(f"âœ… æˆåŠŸè·å–é¡µé¢ ({len(html)} å­—ç¬¦)")
            
            # è§£æHTML
            soup = BeautifulSoup(html, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰PDFé“¾æ¥
            pdf_links = soup.find_all('a', href=lambda href: href and href.endswith('.pdf'))
            print(f"\nğŸ”— æ‰¾åˆ° {len(pdf_links)} ä¸ªPDFé“¾æ¥:")
            
            for i, link in enumerate(pdf_links[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
                href = link.get('href')
                text = link.get_text(strip=True)
                is_individual = spider._is_individual_paper(href) if href else False
                
                print(f"  {i}. {href}")
                print(f"     æ–‡æœ¬: {text}")
                print(f"     å•ç¯‡è®ºæ–‡: {'âœ…' if is_individual else 'âŒ'}")
                print()
            
            # æŸ¥æ‰¾"Paper:"æ–‡æœ¬
            paper_texts = soup.find_all(text=lambda text: text and 'paper:' in text.lower())
            print(f"\nğŸ“ æ‰¾åˆ° {len(paper_texts)} ä¸ª'Paper:'æ–‡æœ¬")
            
            # æµ‹è¯•è®ºæ–‡ä¿¡æ¯æå–
            print(f"\nğŸ” æµ‹è¯•è®ºæ–‡ä¿¡æ¯æå–:")
            extracted_papers = []
            
            for link in pdf_links:
                paper = spider._extract_paper_info_from_link(link, session_url)
                if paper:
                    extracted_papers.append(paper)
            
            print(f"âœ… æˆåŠŸæå– {len(extracted_papers)} ç¯‡è®ºæ–‡ä¿¡æ¯")
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(debug_paper_extraction())
